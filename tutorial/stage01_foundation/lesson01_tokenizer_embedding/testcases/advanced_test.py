"""
L01: Tokenizer ä¸ Embedding - è¿›é˜¶æµ‹è¯•

è¦†ç›– lesson1 çš„è¿›é˜¶å¥‘çº¦ï¼š
- PositionalEncoding: æ­£ä½™å¼¦å…¬å¼ã€åŠ æ³•æ³¨å…¥
- ByteLevelTokenizer: bytes<->unicode åŒå°„ä¸å¯é€†æ€§ï¼ˆåŒ…å«é ASCIIï¼‰
- RoPE: é•¿åºåˆ—/ä½ç²¾åº¦ç¨³å®šæ€§ã€base å‚æ•°å½±å“ã€èŒƒæ•°å®ˆæ’
"""

import torch
import pytest
import sys
import os

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))))

from model.embedding import TokenEmbedding, RoPE, PositionalEncoding
from tokenizer.byte_level_tokenizer import ByteLevelTokenizer


def _reference_sinusoidal_pe(d_model: int, max_len: int, device: torch.device) -> torch.Tensor:
    position = torch.arange(max_len, device=device).unsqueeze(1)  # [max_len, 1]
    div_term = torch.exp(
        torch.arange(0, d_model, 2, device=device, dtype=torch.float32)
        * (-torch.log(torch.tensor(10000.0, device=device)) / d_model)
    )  # [d_model/2]
    pe = torch.zeros(max_len, d_model, device=device, dtype=torch.float32)
    pe[:, 0::2] = torch.sin(position.float() * div_term)
    pe[:, 1::2] = torch.cos(position.float() * div_term)
    return pe


class TestRoPEAdvanced:
    """æµ‹è¯• RoPE è¿›é˜¶ç‰¹æ€§"""

    def test_rope_long_sequence(self):
        """æµ‹è¯• RoPE å¤„ç†é•¿åºåˆ—"""
        batch, heads, seq_len, head_dim = 1, 4, 4096, 32
        rope = RoPE(dim=head_dim, max_position_embeddings=seq_len)
        q = torch.randn(batch, heads, seq_len, head_dim)
        k = torch.randn(batch, heads, seq_len, head_dim)
        pos = torch.arange(seq_len).unsqueeze(0)

        q2, k2 = rope(q, k, pos)
        assert q2.shape == q.shape
        assert k2.shape == k.shape
        assert not torch.isnan(q2).any()
        assert not torch.isinf(q2).any()

    def test_rope_different_bases(self):
        """æµ‹è¯•ä¸åŒ base å‚æ•°çš„å½±å“"""
        batch, heads, seq_len, head_dim = 1, 2, 32, 32
        q = torch.randn(batch, heads, seq_len, head_dim)
        k = torch.randn(batch, heads, seq_len, head_dim)
        pos = torch.arange(seq_len).unsqueeze(0)

        rope1 = RoPE(dim=head_dim, max_position_embeddings=1024, base=10000.0)
        rope2 = RoPE(dim=head_dim, max_position_embeddings=1024, base=1000.0)

        q1, k1 = rope1(q, k, pos)
        q2, k2 = rope2(q, k, pos)

        att1 = torch.matmul(q1, k1.transpose(-1, -2))
        att2 = torch.matmul(q2, k2.transpose(-1, -2))
        # base æ”¹å˜é¢‘ç‡åˆ†å¸ƒï¼Œé€šå¸¸ä¼šå¯¼è‡´æ³¨æ„åŠ›åˆ†æ•°ä¸åŒ
        assert not torch.allclose(att1, att2)

    def test_rope_energy_conservation(self):
        """æµ‹è¯• RoPE èƒ½é‡å®ˆæ’ (æ—‹è½¬ä¸æ”¹å˜å‘é‡é•¿åº¦)"""
        batch, heads, seq_len, head_dim = 2, 4, 64, 32
        rope = RoPE(dim=head_dim, max_position_embeddings=2048)
        q = torch.randn(batch, heads, seq_len, head_dim)
        k = torch.randn(batch, heads, seq_len, head_dim)
        pos = torch.arange(seq_len).unsqueeze(0).repeat(batch, 1)

        q2, k2 = rope(q, k, pos)
        assert torch.allclose(q.norm(dim=-1), q2.norm(dim=-1), atol=1e-4, rtol=1e-4)
        assert torch.allclose(k.norm(dim=-1), k2.norm(dim=-1), atol=1e-4, rtol=1e-4)

    def test_rope_batch_processing(self):
        """æµ‹è¯•æ‰¹é‡å¤„ç†"""
        batch, heads, seq_len, head_dim = 3, 2, 17, 32
        rope = RoPE(dim=head_dim, max_position_embeddings=512)
        q = torch.randn(batch, heads, seq_len, head_dim)
        k = torch.randn(batch, heads, seq_len, head_dim)
        pos = torch.arange(seq_len).unsqueeze(0).repeat(batch, 1)
        q2, k2 = rope(q, k, pos)
        assert q2.shape == q.shape
        assert k2.shape == k.shape


class TestPositionalEncoding:
    """æµ‹è¯• Sinusoidal ä½ç½®ç¼–ç """

    def test_positional_encoding_initialization(self):
        """æµ‹è¯•ä½ç½®ç¼–ç åˆå§‹åŒ–"""
        pe = PositionalEncoding(d_model=32, max_len=128)
        # ä½ç½®è¡¨åº”æ˜¯ bufferï¼Œä¸å‚ä¸æ¢¯åº¦
        buffers = dict(pe.named_buffers())
        assert len(buffers) >= 1
        any_buf = next(iter(buffers.values()))
        assert any_buf.requires_grad is False

    def test_positional_encoding_forward(self):
        """æµ‹è¯•ä½ç½®ç¼–ç å‰å‘ä¼ æ’­"""
        d_model, max_len = 32, 128
        module = PositionalEncoding(d_model=d_model, max_len=max_len)
        x = torch.randn(2, 16, d_model)
        y = module(x)
        assert y.shape == x.shape
        # å·®å€¼åº”ç­‰äºå¯¹åº”ä½ç½®çš„ PE
        ref = _reference_sinusoidal_pe(d_model=d_model, max_len=max_len, device=x.device)[: x.shape[1]]
        delta = (y - x).float()
        assert torch.allclose(delta[0], ref, atol=1e-4, rtol=1e-4)

    def test_positional_encoding_additive(self):
        """æµ‹è¯•ä½ç½®ç¼–ç æ˜¯åŠ æ³•å½¢å¼"""
        d_model = 16
        module = PositionalEncoding(d_model=d_model, max_len=32)
        x = torch.zeros(1, 10, d_model)
        y = module(x)
        # è¾“å…¥ä¸º 0 æ—¶è¾“å‡ºåº”ç­‰äºä½ç½®ç¼–ç æœ¬èº«
        ref = _reference_sinusoidal_pe(d_model=d_model, max_len=32, device=x.device)[:10]
        assert torch.allclose(y[0].float(), ref, atol=1e-4, rtol=1e-4)


class TestTokenEmbeddingAdvanced:
    """æµ‹è¯• TokenEmbedding è¿›é˜¶ç‰¹æ€§"""

    def test_embedding_padding(self):
        """æµ‹è¯• padding token å¤„ç†"""
        vocab_size, hidden_size = 10, 8
        emb = TokenEmbedding(vocab_size=vocab_size, hidden_size=hidden_size)
        input_ids = torch.tensor([[0, 1, 0, 2]], dtype=torch.long)  # å‡è®¾ 0 æ˜¯ pad
        out = emb(input_ids)
        assert out.shape == (1, 4, hidden_size)

    def test_embedding_weight_tying(self):
        """æµ‹è¯•æƒé‡å…±äº« (embedding ä¸ lm_head)"""
        vocab_size, hidden_size = 20, 12
        emb = TokenEmbedding(vocab_size=vocab_size, hidden_size=hidden_size)
        lm_head = torch.nn.Linear(hidden_size, vocab_size, bias=False)

        # å¸¸è§çš„ weight tyingï¼šlm_head.weight ä¸ embedding_table å…±äº«åŒä¸€ä»½å‚æ•°
        lm_head.weight = emb.embedding_table
        assert lm_head.weight is emb.embedding_table

        x = torch.tensor([[1, 2, 3]], dtype=torch.long)
        h = emb(x).sum(dim=1)  # [B, H]
        logits = lm_head(h)
        loss = logits.sum()
        loss.backward()
        assert emb.embedding_table.grad is not None

    def test_embedding_large_vocab(self):
        """æµ‹è¯•å¤§è¯è¡¨"""
        vocab_size, hidden_size = 50000, 32
        emb = TokenEmbedding(vocab_size=vocab_size, hidden_size=hidden_size)
        ids = torch.randint(0, vocab_size, (2, 4), dtype=torch.long)
        out = emb(ids)
        assert out.shape == (2, 4, hidden_size)

class TestRoPENumericalStability:
    """æµ‹è¯• RoPE æ•°å€¼ç¨³å®šæ€§"""

    def test_rope_fp16_stability(self):
        """æµ‹è¯• FP16 ä¸‹çš„æ•°å€¼ç¨³å®šæ€§"""
        batch, heads, seq_len, head_dim = 1, 2, 512, 64
        rope = RoPE(dim=head_dim, max_position_embeddings=seq_len)
        q = torch.randn(batch, heads, seq_len, head_dim, dtype=torch.float16)
        k = torch.randn(batch, heads, seq_len, head_dim, dtype=torch.float16)
        pos = torch.arange(seq_len).unsqueeze(0)
        q2, k2 = rope(q, k, pos)
        assert not torch.isnan(q2).any()
        assert not torch.isinf(q2).any()

    def test_rope_bf16_stability(self):
        """æµ‹è¯• BF16 ä¸‹çš„æ•°å€¼ç¨³å®šæ€§"""
        if not hasattr(torch, "bfloat16"):
            pytest.skip("bfloat16 not available")
        batch, heads, seq_len, head_dim = 1, 2, 512, 64
        rope = RoPE(dim=head_dim, max_position_embeddings=seq_len)
        q = torch.randn(batch, heads, seq_len, head_dim, dtype=torch.bfloat16)
        k = torch.randn(batch, heads, seq_len, head_dim, dtype=torch.bfloat16)
        pos = torch.arange(seq_len).unsqueeze(0)
        q2, k2 = rope(q, k, pos)
        assert not torch.isnan(q2.float()).any()
        assert not torch.isinf(q2.float()).any()


class TestRoPEMathematicalProperties:
    """æµ‹è¯• RoPE æ•°å­¦æ€§è´¨"""

    def test_rope_2d_rotation(self):
        """æµ‹è¯•äºŒç»´æ—‹è½¬"""
        head_dim = 2
        rope = RoPE(dim=head_dim, max_position_embeddings=16, base=10000.0)
        q = torch.tensor([[[[1.0, 0.0]]]])  # [B=1,H=1,T=1,D=2]
        k = torch.tensor([[[[0.0, 1.0]]]])
        pos = torch.tensor([[1]])
        q2, k2 = rope(q, k, pos)
        # æ—‹è½¬åä»åº”æ˜¯æœ‰é™æ•°å€¼
        assert torch.isfinite(q2).all()
        assert torch.isfinite(k2).all()

    def test_rope_invariance_to_absolute_position(self):
        """æµ‹è¯•ç»å¯¹ä½ç½®çš„å¯åŠ æ€§ (ç”¨äºç†è§£ RoPE)"""
        batch, heads, seq_len, head_dim = 1, 1, 8, 32
        rope = RoPE(dim=head_dim, max_position_embeddings=4096)
        q = torch.randn(batch, heads, seq_len, head_dim)
        k = torch.randn(batch, heads, seq_len, head_dim)

        pos_a = torch.arange(seq_len).unsqueeze(0)
        pos_b = (torch.arange(seq_len) + 1234).unsqueeze(0)

        qa, ka = rope(q, k, pos_a)
        qb, kb = rope(q, k, pos_b)
        att_a = torch.matmul(qa, ka.transpose(-1, -2))
        att_b = torch.matmul(qb, kb.transpose(-1, -2))
        assert torch.allclose(att_a, att_b, atol=1e-4, rtol=1e-4)


class TestByteLevelTokenizer:
    def test_bytes_to_unicode_is_bijection(self):
        mapping = ByteLevelTokenizer._create_bytes_to_unicode()
        assert isinstance(mapping, dict)
        assert len(mapping) == 256
        assert len(set(mapping.values())) == 256
        # æ¯ä¸ªæ˜ å°„åº”æ˜¯å•å­—ç¬¦ strï¼ˆGPT-2 é£æ ¼ï¼‰
        assert all(isinstance(v, str) and len(v) == 1 for v in mapping.values())

    def test_bytes_unicode_roundtrip(self):
        tok = ByteLevelTokenizer()
        samples = [
            "hello world",
            "Hello\nworld\t!",
            "ä¸­æ–‡æµ‹è¯•",
            "emojiğŸ™‚ğŸš€",
        ]
        for s in samples:
            u = tok._bytes_to_unicode(s)
            back = tok._unicode_to_bytes(u)
            assert back == s

    def test_encode_decode_is_reversible_without_merges(self):
        # æ„é€ åªåŒ…å« 256 å­—èŠ‚åŸºç¡€ç¬¦å·çš„ vocabï¼ˆæ—  merges ä¹Ÿåº”å¯é€†ï¼‰
        mapping = ByteLevelTokenizer._create_bytes_to_unicode()
        vocab: dict[str, int] = {"<pad>": 0, "<|endoftext|>": 1, "<unk>": 2}
        offset = len(vocab)
        for i in range(256):
            vocab[mapping[i]] = offset + i

        tok = ByteLevelTokenizer(vocab=vocab, merges=[])
        text = "Hello, ä¸­æ–‡ğŸ™‚"
        ids = tok.encode(text, add_special_tokens=False)
        assert all(isinstance(i, int) for i in ids)
        assert tok.unk_token_id not in ids
        assert tok.decode(ids) == text


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
